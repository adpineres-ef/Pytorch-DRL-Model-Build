{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e300f916",
   "metadata": {},
   "source": [
    "# # VRP Variant Solver Comparison (8-Node Example)\n",
    "\n",
    "# We will compare three methods for solving a profit-maximizing, duration-constrained vehicle routing problem variant on an 8-node graph:\n",
    "# 1. Deep Reinforcement Learning (DRL) using DQN.\n",
    "# 2. Mixed-Integer Programming (MIP) for the optimal solution.\n",
    "# 3. A simple Greedy Heuristic (Best Reward/Time Ratio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1170ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import pulp \n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953ee856",
   "metadata": {},
   "source": [
    "# # Define Constants (Adjusted for 8 Nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f8e466e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_NODES = 8\n",
    "ACTION_SPACE_SIZE = NUM_NODES\n",
    "\n",
    "# Problem Constraints (Keep the same duration limits for consistency)\n",
    "DURATION_LIMIT = 60.0\n",
    "DURATION_TOLERANCE = 0.10\n",
    "MIN_DURATION = DURATION_LIMIT * (1 - DURATION_TOLERANCE)\n",
    "MAX_DURATION = DURATION_LIMIT * (1 + DURATION_TOLERANCE)\n",
    "BIG_M_PENALTY = -1e9 # Large negative number for rewards\n",
    "\n",
    "# Use a fixed seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db19180f",
   "metadata": {},
   "source": [
    "# ### Generate Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbb01f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Generating sample data for {NUM_NODES} nodes...\")\n",
    "\n",
    "# Time Matrix: Asymmetric times between 2 and 15 hours\n",
    "time_matrix = np.random.uniform(low=2.0, high=15.0, size=(NUM_NODES, NUM_NODES))\n",
    "# Make slightly asymmetric\n",
    "asymmetry_factor = np.random.uniform(low=0.8, high=1.2, size=(NUM_NODES, NUM_NODES))\n",
    "time_matrix *= asymmetry_factor\n",
    "np.fill_diagonal(time_matrix, 0) # Diagonal time is 0\n",
    "\n",
    "# Reward Matrix: Rewards between 50 and 500\n",
    "# Let's make reward somewhat inversely related to time (shorter = higher reward density)\n",
    "# but with significant noise.\n",
    "reward_matrix = 50 + 450 * np.random.rand(NUM_NODES, NUM_NODES)\n",
    "# Add some inverse time correlation + noise\n",
    "time_factor = 1 / (time_matrix + 1e-6) # Add epsilon to avoid division by zero\n",
    "time_factor_scaled = (time_factor / np.max(time_factor)) * 200 # Scale inverse time effect\n",
    "noise = np.random.uniform(-50, 50, size=(NUM_NODES, NUM_NODES))\n",
    "reward_matrix = np.clip(reward_matrix * 0.5 + time_factor_scaled + noise, 50, 500) # Combine & clip\n",
    "np.fill_diagonal(reward_matrix, 0) # Diagonal reward is 0 (will be penalized later)\n",
    "\n",
    "# Round for clarity\n",
    "time_matrix = np.round(time_matrix, 1)\n",
    "reward_matrix = np.round(reward_matrix, 0)\n",
    "\n",
    "# Create DataFrames for easy viewing\n",
    "time_df = pd.DataFrame(time_matrix, index=range(NUM_NODES), columns=range(NUM_NODES))\n",
    "reward_df = pd.DataFrame(reward_matrix, index=range(NUM_NODES), columns=range(NUM_NODES))\n",
    "\n",
    "print(\"\\nSample Time Matrix (hours):\")\n",
    "print(time_df)\n",
    "print(\"\\nSample Reward Matrix:\")\n",
    "print(reward_df)\n",
    "\n",
    "# Apply Big M penalty to reward matrix diagonal (used by DRL and Heuristic)\n",
    "reward_matrix_penalized = reward_matrix.copy()\n",
    "np.fill_diagonal(reward_matrix_penalized, BIG_M_PENALTY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b23b65",
   "metadata": {},
   "source": [
    "# **Data Description:**\n",
    "# * **Nodes:** 8 locations, indexed 0 through 7.\n",
    "# * **Time Matrix:** Shows the travel time in hours between any two nodes `i` and `j` (`time_matrix[i][j]`). Times are generally between 2 and 15 hours and are slightly asymmetric (travel `i` to `j` might take slightly different time than `j` to `i`). Diagonal is 0.\n",
    "# * **Reward Matrix:** Shows the reward (profit) gained by traveling between nodes `i` and `j` (`reward_matrix[i][j]`). Rewards range roughly from 50 to 500. There's a loose inverse correlation with time (shorter trips *tend* to have higher reward density) plus random noise. Diagonal is 0.\n",
    "#\n",
    "# **Goal Reminder:** For each starting node, find a route (cycle) that starts and ends at that node, maximizes total reward, and has a total duration around 60 hours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d228d8a",
   "metadata": {},
   "source": [
    "## Part 2: DRL Implementation and Training\n",
    "### DRL Hyperparameters and Agent Definition (Using PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17bfa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DRL Hyperparameters (Can potentially reduce episodes/steps for smaller problem)\n",
    "STATE_SIZE = 2 # (current_node_index, time_elapsed_normalized)\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA = 0.95 # Discount factor\n",
    "\n",
    "EPSILON_START = 1.0\n",
    "EPSILON_END = 0.05 # Can end higher for smaller problems\n",
    "EPSILON_DECAY_STEPS = 5000 # Decay faster for smaller problems?\n",
    "\n",
    "BUFFER_SIZE = 10000 # Smaller buffer might be okay\n",
    "BATCH_SIZE = 32 # Smaller batch size\n",
    "\n",
    "NUM_EPISODES = 3000 # Reduced episodes for 8 nodes\n",
    "MAX_STEPS_PER_EPISODE = 50 # Max steps per route attempt\n",
    "TARGET_UPDATE_FREQ = 50 # Update target net more frequently\n",
    "\n",
    "# Rewards / Penalties (Keep the same)\n",
    "RETURN_SUCCESS_BONUS = 100\n",
    "TIME_VIOLATION_PENALTY = -1000\n",
    "INCOMPLETE_PENALTY = -200\n",
    "\n",
    "# PyTorch Device Setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd1775ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q-Network Definition (Same as before)\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64) # Smaller network might suffice\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# DQN Agent Definition (Same DQNAgent_PyTorch class as before)\n",
    "# Ensure the DQNAgent_PyTorch class definition from the previous response is included here\n",
    "class DQNAgent_PyTorch:\n",
    "    def __init__(self, state_size, action_size, learning_rate, gamma, buffer_size, batch_size, device):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.gamma = gamma\n",
    "        self.batch_size = batch_size\n",
    "        self.device = device\n",
    "\n",
    "        # Use the smaller QNetwork\n",
    "        self.policy_net = QNetwork(state_size, action_size).to(self.device)\n",
    "        self.target_net = QNetwork(state_size, action_size).to(self.device)\n",
    "        self.update_target_model()\n",
    "        self.target_net.eval()\n",
    "\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=learning_rate)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.epsilon = EPSILON_START\n",
    "\n",
    "    def update_target_model(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        state_tensor = torch.from_numpy(state).float().unsqueeze(0).to(self.device)\n",
    "        if random.random() <= self.epsilon:\n",
    "            current_node_index = int(state[0])\n",
    "            possible_actions = list(range(self.action_size))\n",
    "            if current_node_index in possible_actions:\n",
    "                 possible_actions.remove(current_node_index)\n",
    "            if not possible_actions:\n",
    "                return random.randrange(self.action_size)\n",
    "            return random.choice(possible_actions)\n",
    "        else:\n",
    "            self.policy_net.eval()\n",
    "            with torch.no_grad():\n",
    "                q_values = self.policy_net(state_tensor)\n",
    "            self.policy_net.train()\n",
    "            q_values_numpy = q_values.cpu().data.numpy()[0]\n",
    "            current_node_index = int(state[0])\n",
    "            if 0 <= current_node_index < self.action_size:\n",
    "                q_values_numpy[current_node_index] = -np.inf\n",
    "            return np.argmax(q_values_numpy)\n",
    "\n",
    "    def replay(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return 0.0\n",
    "        minibatch = random.sample(self.memory, self.batch_size)\n",
    "        states = torch.from_numpy(np.vstack([e[0] for e in minibatch])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e[1] for e in minibatch])).long().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e[2] for e in minibatch])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e[3] for e in minibatch])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e[4] for e in minibatch]).astype(np.uint8)).float().to(self.device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target_q_next = self.target_net(next_states)\n",
    "            max_q_next = target_q_next.max(1)[0].unsqueeze(1)\n",
    "            target_q_values = rewards + (self.gamma * max_q_next * (1 - dones))\n",
    "\n",
    "        current_q_values = self.policy_net(states)\n",
    "        action_q_values = current_q_values.gather(1, actions)\n",
    "        loss = self.loss_function(action_q_values, target_q_values)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        return loss.item()\n",
    "\n",
    "    def decay_epsilon(self, current_step):\n",
    "         self.epsilon = max(EPSILON_END, EPSILON_START - (EPSILON_START - EPSILON_END) * (current_step / EPSILON_DECAY_STEPS))\n",
    "\n",
    "    def load(self, path):\n",
    "        try:\n",
    "             self.policy_net.load_state_dict(torch.load(path, map_location=self.device))\n",
    "             self.update_target_model()\n",
    "             print(f\"Model weights loaded from {path}\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error loading model weights: {e}\")\n",
    "\n",
    "    def save(self, path):\n",
    "        try:\n",
    "             os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "             torch.save(self.policy_net.state_dict(), path)\n",
    "             print(f\"Model weights saved to {path}\")\n",
    "        except Exception as e:\n",
    "             print(f\"Error saving model weights: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5192963f",
   "metadata": {},
   "source": [
    "### DRL Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831eb6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "drl_agent = DQNAgent_PyTorch(state_size=STATE_SIZE,\n",
    "                           action_size=ACTION_SPACE_SIZE,\n",
    "                           learning_rate=LEARNING_RATE,\n",
    "                           gamma=GAMMA,\n",
    "                           buffer_size=BUFFER_SIZE,\n",
    "                           batch_size=BATCH_SIZE,\n",
    "                           device=device)\n",
    "\n",
    "# Training History\n",
    "drl_episode_rewards = []\n",
    "drl_episode_losses = []\n",
    "drl_total_steps = 0\n",
    "drl_start_train_time = time.time()\n",
    "\n",
    "print(\"Starting DRL Training...\")\n",
    "\n",
    "for episode in range(NUM_EPISODES):\n",
    "    start_node = random.randint(0, NUM_NODES - 1) # Train on random start nodes\n",
    "    current_node = start_node\n",
    "    time_elapsed = 0.0\n",
    "    state = np.array([current_node, time_elapsed / MAX_DURATION], dtype=np.float32)\n",
    "\n",
    "    episode_reward = 0\n",
    "    episode_loss_sum = 0\n",
    "    steps_in_episode = 0\n",
    "    done = False\n",
    "\n",
    "    for step in range(MAX_STEPS_PER_EPISODE):\n",
    "        action = drl_agent.act(state)\n",
    "        next_node = action\n",
    "        step_time = time_matrix[current_node][next_node]\n",
    "        # Use penalized reward matrix for DRL training decisions (implicitly via learned Q)\n",
    "        # but store experience based on actual rewards + terminal bonus/penalty\n",
    "        step_reward = reward_matrix[current_node][next_node] # Base reward for the step\n",
    "\n",
    "        next_time_elapsed = time_elapsed + step_time\n",
    "        next_state = np.array([next_node, min(next_time_elapsed, MAX_DURATION) / MAX_DURATION], dtype=np.float32)\n",
    "\n",
    "        terminal_reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Termination checks (Same logic as before)\n",
    "        if next_node == start_node:\n",
    "            if MIN_DURATION <= next_time_elapsed <= MAX_DURATION:\n",
    "                terminal_reward = RETURN_SUCCESS_BONUS\n",
    "                done = True\n",
    "            elif next_time_elapsed < MIN_DURATION:\n",
    "                 terminal_reward = INCOMPLETE_PENALTY\n",
    "                 done = True\n",
    "            else: # > MAX_DURATION\n",
    "                 terminal_reward = TIME_VIOLATION_PENALTY\n",
    "                 done = True\n",
    "        elif next_time_elapsed > MAX_DURATION:\n",
    "            terminal_reward = TIME_VIOLATION_PENALTY\n",
    "            done = True\n",
    "\n",
    "        # Total reward for the experience tuple\n",
    "        total_reward_experience = step_reward + terminal_reward\n",
    "\n",
    "        drl_agent.remember(state, action, total_reward_experience, next_state, done)\n",
    "\n",
    "        state = next_state\n",
    "        current_node = next_node\n",
    "        time_elapsed = next_time_elapsed\n",
    "        episode_reward += step_reward # Track sum of actual step rewards\n",
    "        steps_in_episode += 1\n",
    "        drl_total_steps += 1\n",
    "\n",
    "        drl_agent.decay_epsilon(drl_total_steps)\n",
    "        loss = drl_agent.replay()\n",
    "        if loss > 0: episode_loss_sum += loss\n",
    "        if drl_total_steps % TARGET_UPDATE_FREQ == 0: drl_agent.update_target_model()\n",
    "        if done:\n",
    "            episode_reward += terminal_reward # Add final bonus/penalty for logging\n",
    "            break\n",
    "\n",
    "    drl_episode_rewards.append(episode_reward)\n",
    "    avg_loss = episode_loss_sum / steps_in_episode if steps_in_episode > 0 else 0\n",
    "    drl_episode_losses.append(avg_loss)\n",
    "\n",
    "    if (episode + 1) % (NUM_EPISODES // 10) == 0: # Print progress 10 times\n",
    "         print(f\"DRL Episode: {episode + 1}/{NUM_EPISODES}, Steps: {steps_in_episode}, Total Steps: {drl_total_steps}, Reward: {episode_reward:.0f}, Avg Loss: {avg_loss:.4f}, Epsilon: {drl_agent.epsilon:.3f}\")\n",
    "\n",
    "drl_training_time = time.time() - drl_start_train_time\n",
    "print(f\"\\nDRL Training Finished. Total time: {drl_training_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
